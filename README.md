## 🙌 여러분의 여행을 도와줄 친구, 트래블지니에 오신 것을 환영합니다!
![genie](https://upload.wikimedia.org/wikipedia/en/thumb/0/0c/The_Genie_Aladdin.png/250px-The_Genie_Aladdin.png)   

## ❓ Travel Gene가 뭐예요?   
- **Travel Gene**는 **여행하다(Travel)와 생성형 AI를 뜻하는(Gene)의 합성어에요** 알라딘의 요술램프 지니도 연상이 되나요?
- 마치 지니처럼 여러분들의 여행일정을 소원을 들어주듯이 만들어줄 소프트웨어에요! 좀 더 친숙하게 다가와주세요😀
- 어떤 LLM을 사용할 수 있냐구요!? 특별히 원하는 것이 있으면 트래블지니가 넣어볼게요!!


## 🙋‍♀️ 지니는 무엇을 할 수 있나요?
1. 🗺️ 지니와 대화를 나눠보세요! 여행 장소를 정하는 것부터 시작해볼까요?
2. 🪄 여행 계획을 예쁘게 작성하고 싶을 때, 지니는 요술을 부린답니다.
3. 📋 여러분의 캘린더를 알려주세요, 지니는 여러분의 일정을 관리해줄 수 있어요
4. 📇 여행일정을 친구분들에게도 공유해드릴까요? 물론이죠!
5. 📝 지니를 못 믿으시겠나요? 지니는 자체 인증을 거친 요정들만 있답니다

## 🛠 기술 선택

<details>
<summary>Service Architecture</summary>
서비스의 주 목적은 LLM을 통해 사용자의 여행을 계획해주는 것입니다.


이 때, LLM 서비스는 순수하게 LLM서비스로 동작하고, 인증, 로깅, 예외 상황 처리,라우팅 분리 등의 로직은 따로 백엔드 서버를 분리합니다. 

기능별로 책임을 분리하여 코드별 확장성을 높이고, 유지보수를 용이하게 합니다.

만약 LLM-Heavy한 동작이 들어와 LLM 서비스가 성능적으로 딜레이가 발생한다면, 의도적으로 분리된 구조로 인해 오케스트레이션 시에 전략적인 Scale-out이 가능하게 됩니다.

또한, 만일의 경우 내부 모델을 사용하게 된다면, LLM 서비스는 CPU/GPU/Memory 등 성능적인 이점이 많은 인스턴스를 택하고, 일반 백엔드 서버는 좀 더 가벼운 구조로 가져갈 수도 있습니다.

[Frontend UI] <-> [Backend Server] <-> [LLM Service]
위처럼 세 가지 서비스로 나누어 구성합니다.

</details>

<details>
<summary>LLM Application Library</summary>
선택에 앞서 유용한 두 가지의 LLM 애플리케이션 라이브러리를 비교합니다.

---

**LangChain** 
* LangChain은 LLM 앱 개발에 필요한 구성 요소를 제공합니다.
* 마치 pipeline을 구성하듯이 컴포넌트를 조합하여 구성합니다.
* Chain이라는 이름답게 순차적으로 Chaining을 통해 원하는 결과를 얻을 수 있습니다.
* 복잡한 로직(조건 분기, 반복, 오류 처리)등의 작업이 불편합니다.
* 비교적 구현이 간단하고, 빠르게 적용하기에 적합합니다.

---

**LangGraph**
* LangGraph는 앱의 상태 기반을 통해 구성됩니다.
* 그래프의 node와 edge, StateGraph를 사용하여 상태 머신을 구성합니다.
* node를 다시 방문하거나 순회하여 원하는 결과를 얻을 수 있습니다.
* 복잡한 로직(조건 분기, 반복, 오류 처리)등의 작업이 용이합니다.
* 비교적 구현이 복잡하고, 빠르게 적용하기에 어려움이 있습니다.

---
아래의 이유로 LangGraph를 우선 선택합니다.
* LangChain은 여러 프로젝트에서 가벼히 사용해본 경험이 있지만, LangGraph는 사용해보지 않아, 이번 주제에서 학습해볼 수 있는 기회입니다.
* 지니의 역할 상 여러 상태(계획 작성, 계획 확인, 일정 관리, 일정 공유)를 처리해야 합니다. 이 때, 상태 관리가 유용할 것이라 판단합니다.
* 예를 들어 이런 사용자 요청이 있을 때 LangChain보다 더 적절한 상태값을 가질 수 있습니다. ('이전 계획을 수정해줘', '다른 옵션을 보여줘', '비행기 예약을 하루 미뤄줘')
* 외부 API를 여러 단계로 호출해야할 때 더 적절한 선택이 될 수 있습니다.

---
왜 '우선' 선택인가?
* LangGraph의 초기 도입은 LangChain에 비해 자명하게 러닝 커브가 있습니다.
* 또한 해당 서비스의 역할을 일차원적으로 볼 때, 오버 엔지니어링일 수도 있습니다.
* 이는 초기 구성 멤버가 아닌, 새 멤버 합류 시 초기 생산성의 저하를 가져올 수 있습니다
* 한정된 시간 안에서 새로운 라이브러리의 도입은 도전적일 수 있습니다. 필요 시에 LangChain으로 다시 고려해볼 수 있습니다.

</details>

<details>
<summary>Framework</summary>
LLM Application Library를 선택했으니, 가장 잘 맞는 프레임워크를 선택합니다.

---
**Frontend Framework** 

다음과 같은 이유로 Streamlit을 선택합니다.

* 해당 프로젝트의 주된 Role은 LLM 서비스입니다. 한정된 자원으로 중요도를 볼 때, 프론트엔드 작업은 좀 더 간단하게 구성될 필요가 있습니다.
* 위의 이유로, React나 Next.js와 같은 프레임워크는 무거운 편이라 생각됩니다.
* LangGraph는 프레임워크에 의존적이지 않지만 python 기반으로 택할 것이기 때문에 Streamlit으로 개발하는 것은 언어적인 유사성을 가집니다.
* 또한 가벼운 프레임워크기 때문에 빠르게 적용할 수 있습니다.

---
**Backend Framework**

다음과 같은 이유로 NestJS, FastAPI(LangGraph)를 선택합니다. (개인적인 경향이 묻어있습니다.)
* FastAPI(LangGraph) + FastAPI(Server) + Streamlit을 사용할 때에 얻을 수 있는 언어적 통일성보다, 현재 팀(개인)의 숙련도가 node.js(NestJS)에 맞춰져있습니다. 이는 더 안정적이고 빠른 프로젝트 완성을 꾀할 수 있습니다.
* 의도적으로 백엔드 서버는 중앙 통제 역할을 진행하려 합니다. 백엔드 개발자이기 때문에 백앤드를 중점적으로 구성하고 이를 유연하게 전파할 수 있습니다.
* 예를 들어, DTO를 작성한다고 했을 때, 타입에 더 강점이 있는 typescript에서 DTO를 지정하고 FE와 LLM Service에서는 OpenApi codegen 기능을 사용하여 통일성 있고, 타입에 강한 인터페이스를 유지할 수 있습니다.
* 이는 개발적으로도, 안정성으로도 큰 이익을 취할 수 있습니다.
* 또한 Spring 보다 가볍게 사용할 수 있고 npm 환경을 그대로 사용할 수 있다는 장점이 있습니다.

</details>


<details>
  <summary>UI</summary>
  * Streamlit에서 제공하는 기본 컴포넌트를 적극적으로 사용하고, 되도록이면 AI의 도움을 받습니다.
</details>


<details>
  <summary>IDE</summary>
  * Cursor를 사용합니다. 
  * 이유는 현재 AI 백엔드 개발을 주로 하고 있는데, LangGraph와 프론트 개발 경험은 적습니다.
  * 학습 보다 적절한 아이디어를 통해 AI 개발도구의 도움을 받아 LLM 서비스의 고도화를 이루는 것에 초점을 맞춥니다.
</details>


## 🪔 실행 방법
